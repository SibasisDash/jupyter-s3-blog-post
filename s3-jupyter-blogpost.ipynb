{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNDfMCw0ZNFo"
   },
   "source": [
    "It has been a long time since I've last posted anything. I must admit that it is only partly because I'm busy trying to [finish my PhD in my spare time](http://carolynlangen.com/2017/06/19/academia-to-industry-making-the-switch/). Sometimes I've also felt a bit too lazy to use up what little time I have left over to write a post. But it has been too long now!\n",
    "\n",
    "Lately at my job I've been working a lot with [Amazon Web Services'](https://aws.amazon.com/) (AWS) Simple Storage Solution [(S3)](https://aws.amazon.com/s3/?hp=tile&so-exp=below), which provides cloud-based file storage. I have also been meaning to dive more into using [Jupyter notebooks](http://jupyter.org/), which are very useful in data science. I decided to create the content for this post, which will focus on setting up AWS and using S3, in a Jupyter notebook, which I then converted to HTML and uploaded to my blog. Originally I started to write this post using [Colaboratory](https://colab.research.google.com), which is an online Jupyter extension by Google. However, once I got to the point of accessing S3 via the Python SDK, I realized that I would need to somehow provide my credentials. Given that Colaboratory is still under development, I'm not confident enough that I can securely connect to S3 there, so I switched back to the original Jupyter notebook. You can find a fairly in-depth description of what Jupyter notebooks are and how to use them [here](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook). An important component of making notebooks is writing descriptions in markdown, for which I found [this cheatsheet](https://colab.research.google.com/notebook#fileId=/v2/external/notebooks/markdown_guide.ipynb) to be quite helpful.\n",
    "\n",
    "This notebook is available [on github](https://github.com/cdlangen/jupyter-s3-blog-post)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7uTJtvYyZNFp"
   },
   "source": [
    "# Setting up AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pxBpVh42ZNFr"
   },
   "source": [
    "In order to interact with AWS I first of all need my own instance of AWS. AWS has a [free tier](https://aws.amazon.com/free/) of services. This means that you can use the services for free, up to certain monthly limits. However, once those limits are surpassed, you will be charged for usage. Because of this, AWS requires that you provide payment information upon signing up.\n",
    "\n",
    "I must be honest that I am not completely comfortable having to provide my payment information up front. I would much rather that my AWS resources simply become frozen when I've reached my monthly limit, at least while I am just learning about AWS. But, I do want to experiment, so I ended up providing my payment information. I set up the [billing alarm](http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/free-tier-alarms.html), which will notify me when I reach the free limit. As far as I know, there is no way to automatically freeze resources that will incur charges, so I guess for now I just need to be extra careful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cz7jRCDKZNFs"
   },
   "source": [
    "## Working with S3 web interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "luWY8NNtZNFt"
   },
   "source": [
    "To start using S3 I used the web interface to set it up and load a sample file, following [these directions](https://aws.amazon.com/getting-started/tutorials/backup-files-to-amazon-s3/). I followed the directions exactly, which was straightforward. While working with this interface is nice, what is perhaps more interesting for programmers is the command line interface (CLI) and programmatic access (I will focus on Python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xh3rPh5WgeWi"
   },
   "source": [
    "## Working with S3 via the CLI and Python SDK\n",
    "\n",
    "Before it is possible to work with S3 programmatically, it is necessary to set up an AWS IAM User. [This guide](https://aws.amazon.com/getting-started/tutorials/backup-to-s3-cli/) shows how to do that, plus other steps necessary to install and configure AWS. To work with with Python SDK, it is also necessary to install boto3 (which I did with the command `pip install boto3`). Below I will demonstrate the SDK, along with the equivalent commands in the CLI. First, however, we need to import boto3 and initialize and S3 object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 168,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1247,
     "status": "error",
     "timestamp": 1510159573066,
     "user": {
      "displayName": "Carolyn Langen",
      "photoUrl": "//lh3.googleusercontent.com/-Yq0ind23iGY/AAAAAAAAAAI/AAAAAAAACEA/rKv6KAVDHRE/s50-c-k-no/photo.jpg",
      "userId": "106654000967056995838"
     },
     "user_tz": -60
    },
    "id": "LOGm4hJmhhMJ",
    "outputId": "c6c634d2-bdd1-4cf7-b7f2-d10e2ec67ed2"
   },
   "outputs": [],
   "source": [
    "import boto3, os\n",
    "\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You can also do `s3 = boto3.client('s3')`, but some functionality won't be possible (like s3.Bucket()).\n",
    "\n",
    "### Creating a bucket\n",
    "Bucket names need to be globally unique, meaning that no two buckets can have the same name, not even when they are owned by different users. \n",
    "#### With the CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket: demo-bucket-cdl\r\n"
     ]
    }
   ],
   "source": [
    "! aws s3 mb s3://demo-bucket-cdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3.Bucket(name='demo-bucket-cdl2')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.create_bucket(Bucket='demo-bucket-cdl2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload file\n",
    "I first made a small test file and made a copy with this command \n",
    "`echo test file > test.txt`\n",
    "#### With the CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 10 Bytes/10 Bytes (15 Bytes/s) with 1 file(s) remaining\r",
      "upload: ./test.txt to s3://demo-bucket-cdl/test.txt              \r\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp test.txt s3://demo-bucket-cdl/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command above copies a file, but you can also move files using `mv`. I haven't explicitly included the filename on the S3 end, which will result in the file having the same name as the original file. You can also explicitly tell S3 what the file name should be, including subfolders without creating the subfolders first (in fact, subfolders to not exist on S3 in the way that they do in other file systems). \n",
    "\n",
    "#### With the SDK\n",
    "Let's upload the file twice, one in a subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3.meta.client.upload_file('test.txt', 'demo-bucket-cdl2', 'test2.txt')\n",
    "s3.meta.client.upload_file('test.txt', 'demo-bucket-cdl2', 'subdir/test3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if s3 was a client instead of a resource, the command becomes `s3.upload_file('test.txt', 'demo-bucket-cdl2', 'test2.txt')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting objects\n",
    "#### With the CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://demo-bucket-cdl/test.txt\r\n"
     ]
    }
   ],
   "source": [
    "! aws s3 rm s3://demo-bucket-cdl/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the SDK\n",
    "We can delete the object via the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'HTTPHeaders': {'date': 'Wed, 22 Nov 2017 19:54:40 GMT',\n",
       "   'server': 'AmazonS3',\n",
       "   'x-amz-id-2': 'c/OUm5MoDWUZgVmdf3ojOdTfE725yJfQ0Fx4Ye74vTQJ+7fCVKwQIPqweIqgHw6al9Wc9+N77gc=',\n",
       "   'x-amz-request-id': '0B084EEA55655C6B'},\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HostId': 'c/OUm5MoDWUZgVmdf3ojOdTfE725yJfQ0Fx4Ye74vTQJ+7fCVKwQIPqweIqgHw6al9Wc9+N77gc=',\n",
       "  'RequestId': '0B084EEA55655C6B',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.meta.client.delete_object(Bucket=\"demo-bucket-cdl2\", Key=\"test2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can also delete an object that has already been retrieved from s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'HTTPHeaders': {'date': 'Wed, 22 Nov 2017 19:54:40 GMT',\n",
       "   'server': 'AmazonS3',\n",
       "   'x-amz-id-2': '7KX1QmMEiwxwCC3D/FlydH3hhF3AM+nCyy8vj6LrlSQN8Fs9GL1kmDCKAtJ45av/l+rKr0UqMJ0=',\n",
       "   'x-amz-request-id': '188479D5427A17D6'},\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HostId': '7KX1QmMEiwxwCC3D/FlydH3hhF3AM+nCyy8vj6LrlSQN8Fs9GL1kmDCKAtJ45av/l+rKr0UqMJ0=',\n",
       "  'RequestId': '188479D5427A17D6',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = s3.Object(\"demo-bucket-cdl2\", \"subdir/test3.txt\")\n",
    "obj.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now all of the objects have been deleted, so let's create a few more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3.meta.client.upload_file('test.txt', 'demo-bucket-cdl', 'test.txt')\n",
    "s3.meta.client.upload_file('test.txt', 'demo-bucket-cdl', 'subdir/test.txt')\n",
    "s3.meta.client.upload_file('test.txt', 'demo-bucket-cdl2', 'test2.txt')\n",
    "s3.meta.client.upload_file('test.txt', 'demo-bucket-cdl2', 'subdir/test2.txt')\n",
    "s3.meta.client.upload_file('test.txt', 'demo-bucket-cdl2', 'test3.txt')\n",
    "s3.meta.client.upload_file('test.txt', 'demo-bucket-cdl2', 'subdir/subir/test3.txt')\n",
    "s3.meta.client.upload_file('test.txt', 'demo-bucket-cdl2', 'subidr2/test3.txt')\n",
    "s3.meta.client.upload_file('test.txt', 'demo-bucket-cdl2', 'subdir2/subdir/subdir/test2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving and copying objects\n",
    "#### With the CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "move: s3://demo-bucket-cdl/test.txt to s3://demo-bucket-cdl/moved.txt\n"
     ]
    }
   ],
   "source": [
    "! aws s3 mv s3://demo-bucket-cdl/test.txt s3://demo-bucket-cdl/moved.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 10 Bytes/10 Bytes (16 Bytes/s) with 1 file(s) remaining\r",
      "copy: s3://demo-bucket-cdl/subdir/test.txt to s3://demo-bucket-cdl/test.txt\r\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp s3://demo-bucket-cdl/subdir/test.txt s3://demo-bucket-cdl/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the SDK\n",
    "boto3 doesn't appear to have a move function, but it can be easily accomplished by first copying the file, and then deleting the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'HTTPHeaders': {'date': 'Wed, 22 Nov 2017 19:54:47 GMT',\n",
       "   'server': 'AmazonS3',\n",
       "   'x-amz-id-2': 'SSoeJd5OO9oLbglRYfgQjbGOZ+9VpKYt/mH2KZyOsqY8IKG4Q6fMF6KGGi6Qz/w4bXXjY/Oms4s=',\n",
       "   'x-amz-request-id': '352FE9BEB8F61026'},\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HostId': 'SSoeJd5OO9oLbglRYfgQjbGOZ+9VpKYt/mH2KZyOsqY8IKG4Q6fMF6KGGi6Qz/w4bXXjY/Oms4s=',\n",
       "  'RequestId': '352FE9BEB8F61026',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.Object('demo-bucket-cdl2','moved2.txt').copy_from(CopySource='demo-bucket-cdl2/test2.txt')\n",
    "s3.Object('demo-bucket-cdl2','test2.txt').delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing buckets\n",
    "#### With the CLI\n",
    "The following command can be used to see which buckets you have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-22 20:54:35 demo-bucket-cdl\r\n",
      "2017-11-22 20:54:36 demo-bucket-cdl2\r\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command recursively shows the files in the specified bucket (though the recursive option is not so useful in this case, given that there is only one file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-22 20:54:44         10 moved.txt\r\n",
      "2017-11-22 20:54:40         10 subdir/test.txt\r\n",
      "2017-11-22 20:54:46         10 test.txt\r\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls demo-bucket-cdl/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to see if I could recursively see all objects in all buckets with just one command. Apparently not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-22 20:54:35 demo-bucket-cdl\r\n",
      "2017-11-22 20:54:36 demo-bucket-cdl2\r\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the SDK\n",
    "First, to see which buckets are available to you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo-bucket-cdl\n",
      "demo-bucket-cdl2\n"
     ]
    }
   ],
   "source": [
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the equivalent of the CLI 'ls' command with '--recursive'. As you can see, it iterative rather than recursive. This is because objects in S3 aren't stored in a directory structure. Each object belongs to a bucket, and has a key which identifies it. When the bucket name and object key are combined you get something that looks like a file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo-bucket-cdl2/moved2.txt\n",
      "demo-bucket-cdl2/subdir/subir/test3.txt\n",
      "demo-bucket-cdl2/subdir/test2.txt\n",
      "demo-bucket-cdl2/subdir2/subdir/subdir/test2.txt\n",
      "demo-bucket-cdl2/subidr2/test3.txt\n",
      "demo-bucket-cdl2/test3.txt\n"
     ]
    }
   ],
   "source": [
    "for obj in s3.Bucket(name='demo-bucket-cdl2').objects.all():\n",
    "    print(os.path.join(obj.bucket_name, obj.key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show all objects of all buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo-bucket-cdl/moved.txt\n",
      "demo-bucket-cdl/subdir/test.txt\n",
      "demo-bucket-cdl/test.txt\n",
      "demo-bucket-cdl2/moved2.txt\n",
      "demo-bucket-cdl2/subdir/subir/test3.txt\n",
      "demo-bucket-cdl2/subdir/test2.txt\n",
      "demo-bucket-cdl2/subdir2/subdir/subdir/test2.txt\n",
      "demo-bucket-cdl2/subidr2/test3.txt\n",
      "demo-bucket-cdl2/test3.txt\n"
     ]
    }
   ],
   "source": [
    "for bucket in s3.buckets.all():\n",
    "    for obj in bucket.objects.all():\n",
    "        print(os.path.join(obj.bucket_name, obj.key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursively and selectively move, copy, and delete files\n",
    "#### With the CLI\n",
    "We can recursively move/copy all files in a given bucket to another folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 10 Bytes/30 Bytes (14 Bytes/s) with 3 file(s) remaining\r",
      "copy: s3://demo-bucket-cdl/subdir/test.txt to s3://demo-bucket-cdl/backup/subdir/test.txt\r\n",
      "Completed 10 Bytes/30 Bytes (14 Bytes/s) with 2 file(s) remaining\r",
      "Completed 20 Bytes/30 Bytes (29 Bytes/s) with 2 file(s) remaining\r",
      "copy: s3://demo-bucket-cdl/moved.txt to s3://demo-bucket-cdl/backup/moved.txt\r\n",
      "Completed 20 Bytes/30 Bytes (29 Bytes/s) with 1 file(s) remaining\r",
      "Completed 30 Bytes/30 Bytes (44 Bytes/s) with 1 file(s) remaining\r",
      "copy: s3://demo-bucket-cdl/test.txt to s3://demo-bucket-cdl/backup/test.txt\r\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp s3://demo-bucket-cdl/ s3://demo-bucket-cdl/backup/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also move/copy a subset of files, but to do so we need to use `include` and `exclude` parameters. Let's use `dryrun` to see what will happen without making any changes yet.\n",
    "\n",
    "(Note in the commands below that the * needs to be escaped in an iPython notebook in order for it to be interpreted properly in the shell command.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(dryrun) move: s3://demo-bucket-cdl/moved.txt to s3://demo-bucket-cdl/moved/moved.txt\r\n",
      "(dryrun) move: s3://demo-bucket-cdl/subdir/test.txt to s3://demo-bucket-cdl/moved/subdir/test.txt\r\n",
      "(dryrun) move: s3://demo-bucket-cdl/test.txt to s3://demo-bucket-cdl/moved/test.txt\r\n"
     ]
    }
   ],
   "source": [
    "! aws s3 mv s3://demo-bucket-cdl/ s3://demo-bucket-cdl/moved/ --include \\*test\\*.txt --exclude backup/* --recursive --dryrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous command did not work as expected (i.e. it should not have moved the moved.txt file). That's because `include` and `exclude` are applied sequentially, and the starting state is from all files in `s3://demo-bucket-cdl/`. In this case, all six files that are in demo-bucket-cdl were already included, so the `include` parameter effectively did nothing and the `exclude` excluded the backup folder.\n",
    "\n",
    "Let's try again, first excluding all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(dryrun) move: s3://demo-bucket-cdl/subdir/test.txt to s3://demo-bucket-cdl/moved/subdir/test.txt\r\n",
      "(dryrun) move: s3://demo-bucket-cdl/test.txt to s3://demo-bucket-cdl/moved/test.txt\r\n"
     ]
    }
   ],
   "source": [
    "! aws s3 mv s3://demo-bucket-cdl/ s3://demo-bucket-cdl/moved/ --exclude \\* --include \\*test\\*.txt --exclude backup/* --recursive --dryrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same principles apply for the delete command as well.\n",
    "\n",
    "#### With the SDK\n",
    "As far as I know, including and excluding files is a manual process in the SDK. But doing it yourself is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo-bucket-cdl2/subdir/subir/test3.txt\n",
      "demo-bucket-cdl2/subdir/test2.txt\n",
      "demo-bucket-cdl2/subdir2/subdir/subdir/test2.txt\n",
      "demo-bucket-cdl2/subidr2/test3.txt\n",
      "demo-bucket-cdl2/test3.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "objs = [os.path.join(obj.bucket_name, obj.key) \n",
    "        for obj in s3.Bucket(name='demo-bucket-cdl2').objects.all() \n",
    "        if re.match(\".*test.*\\.txt\",obj.key)]\n",
    "print(\"\\n\".join(objs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting buckets\n",
    "#### With the CLI\n",
    "Since the bucket isn't empty, we need to use the ` --force` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://demo-bucket-cdl/subdir/test.txt\n",
      "delete: s3://demo-bucket-cdl/backup/test.txt\n",
      "delete: s3://demo-bucket-cdl/backup/moved.txt\n",
      "delete: s3://demo-bucket-cdl/backup/subdir/test.txt\n",
      "delete: s3://demo-bucket-cdl/moved.txt\n",
      "delete: s3://demo-bucket-cdl/test.txt\n",
      "remove_bucket: demo-bucket-cdl\n"
     ]
    }
   ],
   "source": [
    "! aws s3 rb s3://demo-bucket-cdl --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the SDK\n",
    "The bucket needs to be manually emptied before it can be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'HTTPHeaders': {'date': 'Wed, 22 Nov 2017 19:55:02 GMT',\n",
       "   'server': 'AmazonS3',\n",
       "   'x-amz-id-2': 'soQ/fzy0WW/JED64l3ippaj63ihz+qlU2z/SFgF8QJd42/QXn67tH1XHfO2jno/CeGHygevYpFY=',\n",
       "   'x-amz-request-id': 'EB58B94E49D8AD23'},\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HostId': 'soQ/fzy0WW/JED64l3ippaj63ihz+qlU2z/SFgF8QJd42/QXn67tH1XHfO2jno/CeGHygevYpFY=',\n",
       "  'RequestId': 'EB58B94E49D8AD23',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = s3.Bucket('demo-bucket-cdl2')\n",
    "\n",
    "# empty the bucket\n",
    "for key in bucket.objects.all():\n",
    "    key.delete()\n",
    "    \n",
    "# then delete it\n",
    "bucket.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last remarks on S3\n",
    "I found the CLI and SDK for S3 quite easy to use. Figuring out how to set up my own S3 instance took some time, but the documentation was thorough and accurate. I'm looking forward to using S3 more in the future, but I am still a bit wary about going over the free limits. I checked my usage from writing this post. Here are the results (keep in mind that I ran these commands several times while writing this notebook):\n",
    "\n",
    "![Screen grab of S3 usage](http://carolynlangen.com/wp-content/uploads/2017/11/Screen-Shot-2017-11-22-at-21.18.26.png \"Screen grab of S3 usage\")\n",
    "\n",
    "# Posting this notebook to WordPress\n",
    "I followed [these instructions](http://www.mianchen.com/wordpress-blogging-with-jupyter-notebook-in-five-simple-steps/) to convert this notebook to html and post it to WordPress. The main difference is that I didn't need to complete step 2 (downloading an iPython notebook) since I was already working on my own machine, and in step 3 the parameters were formatted slightly differently. I ran step 3 like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook s3-jupyter-blogpost.ipynb to html\n",
      "[NbConvertApp] Writing 48203 bytes to s3-jupyter-blogpost.html\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert s3-jupyter-blogpost.ipynb --to html --template basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7JQ31jPXb9Yv"
   },
   "source": [
    "I also had to add this to the css, since (1) the posted solution added anchor points that I didn't like, and (2) the default css had a dark background that made it hard to read:\n",
    "\n",
    "`\n",
    "a.anchor-link{display:none}\n",
    "\n",
    ".entry-content code {\n",
    "    background-color: rgba(0, 0, 0, 0);\n",
    "}`\n",
    "\n",
    "I ignored the advice about suppressing syntax highlighting in the `<pre>` tag, because I don't really mind the different syntax coloring that is generated by the plugin that I use. \n",
    "\n",
    "# What's the verdict on Colaboratory?\n",
    "\n",
    "My first impressions of Colaboratory are quite positive, even though I needed to switch to standard Jupyter when I realised that I needed to connect to my AWS resources. Nevertheless, I did see some of the added value of colaboratory already. When I got an exception from running the command `import boto3`, a button appeared with the label \"Search Stack Overflow\". The button did, indeed, search stack overflow for this exception. Handy! It is really nice that Google offers this service online, but of course that means that it is limited to the packages that are available in the platform. I also assume that the platform doesn't support working offline, which would require a local installation of Jupyter.\n",
    "\n",
    "Colaboratory is easy to use, has a nice interface and seems a bit more intuitive than the original interface for Jupyter notebooks. It also has several features that I haven't yet seen in the origina Jupyter. It is really handy that markdown text sections have a preview which is periodically updated as you type. There is a bit of a delay before new text shows in the preview, but this is already much quicker than in Jupyter, which requires that you compile the cell to see how the text looks. One feature that I wish they had (or at least I don't think is available) is to add markdown tags for formatting similar to what you have in document editors (for example, Ctrl+I could automatically wrap selected text in asterisks to make it *italic*)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "s3-jupyter-blogpost.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
